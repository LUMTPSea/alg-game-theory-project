{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Game Theory Class Project - CSCI 7000\n",
    "\n",
    "----\n",
    "### Notes\n",
    "\n",
    "- Primary Paper: https://arxiv.org/pdf/1910.03094.pdf\n",
    "- Helpful Resources:\n",
    "    - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&rep=rep1&type=pdf\n",
    "    - http://modelai.gettysburg.edu/2013/cfr/cfr.pdf\n",
    "    - https://www.quora.com/What-is-an-intuitive-explanation-of-counterfactual-regret-minimization\n",
    "    - https://int8.io/counterfactual-regret-minimization-for-poker-ai/\n",
    "    - https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html\n",
    "    - Regret Matching: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.4262&rep=rep1&type=pdf\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "- Notes for _Async updates with bandit feedback_ section.\n",
    "    - **Difference** : For CFR or any no-regret algorithm, one needs the feedback of all the actions which are not chosen as well. Bandit ones do not. One can use it when designing an experiment acc to need.\n",
    "    - **Research Direction**: Can we use SARSA type or UCB updates instead of normal bandit?\n",
    "    - what is importance sampling and what effect does it bring to the experiment?\n",
    "    - **Research Direction**: on-policy (given in the paper) vs off-policy RL type?\n",
    "        - \" _It would be natural to adapt LONR\n",
    "to the on-policy RL setting by replacing the no-regret algorithm\n",
    "with a multi-armed bandit one. This type of result has previously\n",
    "been obtained for normal-form games [5]_ \"\n",
    "   \n",
    "----\n",
    "\n",
    "- Notes for Basic _Experiments_ section.\n",
    "    - **Research Direction**: Normal-form games only? can it be extended? (https://vknight.org/Year_3_game_theory_course/Content/Chapter_02-Normal_Form_Games/) (https://en.wikipedia.org/wiki/Normal-form_game) \n",
    "    - can try the theoretical approach as well:\n",
    "        - **Action Item**:  \" _For LONR-A, while the theory requires states be chosen for update uniformly at random, we instead\n",
    "run it on policy. (We add a small probability of a random action, 0.1,\n",
    "to ensure adequate exploration.) Our results show that empirically\n",
    "this does not prevent convergence._ \"\n",
    "    - Need to explore the RM settings in general. Can modify it.\n",
    "    - NoSDE games: A extension of online MDPs to cater to multi agents. Problem is that there are *multiple* randomized policies which result in the state of equil. when q-values are identical for all the agents. _There is a sense of cycle when finding equil._\n",
    "    \n",
    "    - **Doubt** : \" _In this instance, when player 1\n",
    "sends, player 2 then prefers to send. This causes player 1 to prefer\n",
    "to keep, which in turn causes player 2 to prefer to keep. Player 1\n",
    "then prefers to send and the cycle repeats_ \" ??\n",
    "    - **Action Item:** \" _Not shown but important is that each also is\n",
    "converging to the equilibrium Q\n",
    "âˆ—\n",
    "in the average Q values._ \" .\n",
    "\n",
    "    - **Research Direction**: \" _This result highlights NoSDE games as a setting where it would\n",
    "be interesting to theoretically study last iterate convergence in between simple normal form games [4, 41] and rich, complex settings\n",
    "such as GANs [14]._ \"\n",
    "    - **Research Direction**: \" _This simultaneously shows a\n",
    "negative and positive result: increased optimism is not known to\n",
    "work or be required in any other settings. Theoretically exploring\n",
    "this phenomenon is an interesting direction for future work._ \"\n",
    "    - Dive a bit deep into the LONR-A with OMWU vs. RM++. How does it work? what is the algo? (Fig 4(b), (c))\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "- List of the Algorithms Used\n",
    "    - Multiplicative Wgts. Update (also Optimistic MWU)\n",
    "    - Regret Matching (also RM+)\n",
    "    - Discounted CFR\n",
    "    - **New Proposal in this paper**: RM++\n",
    "    - **New Proposal in this paper**: LONR-V / LONR-A\n",
    "    \n",
    "-----\n",
    "\n",
    "- The Soccer Game Experiment\n",
    "    - Friendly Markov game without the complexity of NoSDE games.\n",
    "    - 2 player zero-sum soccer game\n",
    "    - Use any regret minimizers\n",
    "    - learning is successful despite\n",
    "        - for LONR-V: few algs having no-absolute-regret (what was this again?) property\n",
    "        - for LONR-A: on policy state selection\n",
    "----\n",
    "\n",
    "- Future Research Directions with big ideas\n",
    "    - Read the conclusion in the paper. 4-5 ideas stated.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
